Metadata-Version: 2.4
Name: airflow-dags
Version: 0.1.0
Summary: Airflow DAG development with Spark and Iceberg
Requires-Python: >=3.11
Description-Content-Type: text/markdown
Requires-Dist: apache-airflow==3.0.1
Requires-Dist: pyspark==3.5.3
Requires-Dist: pyiceberg==0.10.0
Requires-Dist: pyarrow>=17.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.4.0; extra == "dev"
Requires-Dist: pytest-cov>=4.1.0; extra == "dev"
Requires-Dist: mypy>=1.7.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"

# Airflow DAG Development

Boilerplate for Apache Airflow DAG development with Spark and Iceberg integration.

## Tech Stack

- **Apache Airflow**: 3.0.1 (latest stable)
- **PySpark**: 3.5.3
- **Apache Iceberg**: 1.7.0 (via Spark runtime)
- **PyIceberg**: 0.10.0 (latest)
- **Python**: 3.11+

## Setup

```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment with Python 3.11
uv venv --python 3.11

# Activate environment
source .venv/bin/activate

# Install dependencies
uv sync --all-extras
```

## Key Changes for Airflow 3.0

Airflow 3.0 introduces several breaking changes:

1. **Schedule Parameter**: Use `schedule` instead of `schedule_interval` in DAG definitions
2. **Timetable API**: Access schedule via `dag.timetable` instead of `dag.schedule_interval`
3. **Task ID Inference**: TaskFlow API automatically infers task IDs from function names

## Spark and Iceberg Integration

This project uses:
- **Spark 3.5.3** with Iceberg 1.7.0 runtime for Scala 2.12
- **PyIceberg** for Python-native Iceberg catalog operations
- Hadoop-based local catalog for development/testing

### Iceberg Table Operations

```python
# Write to Iceberg table with Spark
df.writeTo("local.default.my_table").createOrReplace()

# Read from Iceberg table
df = spark.table("local.default.my_table")
```

## Development

```bash
# Run tests
uv run pytest tests/ -v

# Code quality checks
uv run ruff check .
uv run ruff format .
uv run mypy .

# Run all checks at once
uv run ruff check . && uv run ruff format --check . && uv run mypy . && uv run pytest
```

## Project Structure

- `dags/` - Airflow DAG definitions
- `tests/` - Unit tests for DAGs and tasks
- `config/` - Configuration files
- `pyproject.toml` - Dependencies and tool configuration

## Usage

1. Add your DAGs to the `dags/` directory
2. Write unit tests in `tests/`
3. Run quality checks before committing
4. Point Airflow's `dags_folder` to this project's `dags/` directory
